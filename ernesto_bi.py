# -*- coding: utf-8 -*-
"""Ernesto_BI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OpGY5G86eErl1bdrmH4B_bDyeqYsMQL6

# STEP 1:  Data Selection

1. Use MongoDB to load the JSON files and query the database.
2. Filter reviews based on specific criteria (e.g., date, location, price range, …).
3. Extract the text and rating fields for sentiment analysis.

Each file is referred to rewievs on a specific Hotel. e.g. file 77798 contains all the rewievs on Hotel 77798
"""

!pip install pymongo
!pip install "pymongo[srv]"

import pymongo
from pymongo.mongo_client import MongoClient
from pymongo.server_api import ServerApi
uri = "mongodb+srv://Ernesto:Pedrito98!@cluster1.wi5cp.mongodb.net/"
# create a new client and connect to the server
client = MongoClient(uri, server_api=ServerApi('1'))
# ping to confirm a successful connection
try:
    client.admin.command('ping')
    print("Pinged your deployment. You successfully connected to MongoDB!")
except Exception as e:
    print(e)

# connect to drive and load the dataset

from pymongo.mongo_client import MongoClient
from pymongo.server_api import ServerApi
from google.colab import drive
import json
import os

# mount Google Drive
drive.mount('/content/drive')


# JSON files path
path_to_json = "/content/drive/MyDrive/Business_Intelligence/BusinessIntel/DataErnesto/json"

# connection to MongoDB
client = MongoClient(uri, server_api=ServerApi('1'))

# test connection
try:
    client.admin.command('ping')
    print("Pinged your deployment. You successfully connected to MongoDB!")
except Exception as e:
    print(e)

# specify collection and database
db = client['tripadvisorProva']
collection = db['reviews']


# load JSON files in the collection

for file in os.listdir(path_to_json):
    if file.endswith('.json'):
        with open(os.path.join(path_to_json, file)) as f:
            data = json.load(f)
            if isinstance(data, list):
                collection.insert_many(data)
            else:
                collection.insert_one(data)

# verify
print(f"number of loaded reviews: {collection.count_documents({})}")

# list of files
files = os.listdir(path_to_json)
print("directory contents 'json':", files)

# what is in file 77798 ?
with open(os.path.join(path_to_json, '77798.json'), 'r') as f:
    data = json.load(f)
    print("file content:", data)

# number of hotels in the dataset
num_hotels = collection.count_documents({})
print(f"number of hotels in the dataset: {num_hotels}")

# available fields in an instance
file_path = os.path.join(path_to_json, '77798.json')

# load file content
with open(file_path, 'r') as f:
    data = json.load(f)

print("available fields in an instance:")
if isinstance(data, list):
    for key in data[0].keys():
        print(key)
else:
    for key in data.keys():
        print(key)

# print some reviews

if 'Reviews' in data:
    print("'Reviews' content:")
    for idx, review in enumerate(data['Reviews'][:5]):  # show only the first five
        print(f"\nReview {idx + 1}:")
        for key, value in review.items():
            print(f"{key}: {value}")
else:
    print("no field 'Reviews' found.")

# print HotelInfo details
if 'HotelInfo' in data:
    print("'HotelInfo' content:")
    for key, value in data['HotelInfo'].items():
        print(f"{key}: {value}")
else:
    print("'HotelInfo' not found.")

# number of hotels with "Price" fiels present
hotels_with_price = collection.count_documents({"HotelInfo.Price": {"$exists": True, "$ne": ""}})
print(f"number of hotels with 'Price' fiels present: {hotels_with_price}")

# Numero di hotel che non hanno il campo "Price" o che hanno un valore non definito o pari a 0
hotels_without_price = collection.count_documents({"$or": [{"HotelInfo.Price": {"$exists": False}}, {"HotelInfo.Price": ""}]})
print(f"number of hotels with 'Price' fiels NOT present: {hotels_without_price}")

# histogram to plot number of hotels per price range
import re
import matplotlib.pyplot as plt

# first I compute the average price
def parse_price_range(price_str):
    if not price_str or not isinstance(price_str, str):
        return None
    price_match = re.findall(r'\$([\d,]+)', price_str)
    if not price_match or len(price_match) < 1:
        return None
    # convert in numbers and compute the average
    prices = [int(price.replace(',', '')) for price in price_match]
    return sum(prices) / len(prices)

# extract prices from hotels
price_data = collection.find({"HotelInfo.Price": {"$exists": True, "$ne": ""}}, {"HotelInfo.Price": 1})
prices = [parse_price_range(hotel["HotelInfo"]["Price"]) for hotel in price_data]
prices = [p for p in prices if p is not None]

# division in price category
price_ranges = {
    "0–150": sum(1 for p in prices if p <= 150),
    "150–300": sum(1 for p in prices if 150 < p <= 300),
    "300–500": sum(1 for p in prices if 300 < p <= 500),
    "over 500": sum(1 for p in prices if p > 500)
}

# plot the histogram
plt.bar(price_ranges.keys(), price_ranges.values(), color='skyblue')
plt.xlabel("price ranges ($)")
plt.ylabel("hotels' number")
plt.title("hotels' distribution per price range")
plt.show()

# extract the right number


price_data = collection.find({"HotelInfo.Price": {"$exists": True, "$ne": ""}}, {"HotelInfo.Price": 1})
prices = [parse_price_range(hotel["HotelInfo"]["Price"]) for hotel in price_data]
prices = [p for p in prices if p is not None]  # Rimuovi eventuali valori non validi

# range division
price_ranges = {
    "from 0 to 150": sum(1 for p in prices if p <= 150),
    "from 150 to 300": sum(1 for p in prices if 150 < p <= 300),
    "from 300 to 500": sum(1 for p in prices if 300 < p <= 500),
    "over 500": sum(1 for p in prices if p > 500)
}

# print results
for range_name, count in price_ranges.items():
    print(f"{range_name}: {count} hotels")

# average rating per price range


# extract data from hotels
hotels = collection.find({"HotelInfo.Price": {"$exists": True, "$ne": ""}, "Reviews": {"$exists": True, "$ne": []}}, {"HotelInfo.Price": 1, "Reviews.Ratings.Overall": 1})
price_ratings = {"0–150": [], "150–300": [], "300–500": [], "over 500": []}

for hotel in hotels:
    # compute average
    price = parse_price_range(hotel["HotelInfo"]["Price"])
    if price is None:
        continue

    # get ratings "Overall"
    overall_ratings = [
        float(review["Ratings"]["Overall"])
        for review in hotel.get("Reviews", [])
        if "Ratings" in review and "Overall" in review["Ratings"]
    ]

    # compute avg rating
    if overall_ratings:
        avg_rating = sum(overall_ratings) / len(overall_ratings)

        # assign range
        if price <= 150:
            price_ratings["0–150"].append(avg_rating)
        elif 150 < price <= 300:
            price_ratings["150–300"].append(avg_rating)
        elif 300 < price <= 500:
            price_ratings["300–500"].append(avg_rating)
        else:
            price_ratings["over 500"].append(avg_rating)

# compute average rating per price range
for range_name, ratings in price_ratings.items():
    if ratings:
        avg_rating = sum(ratings) / len(ratings)
        print(f"average rating per price range {range_name}: {avg_rating:.2f}")
    else:
        print(f"average rating per price range {range_name}: NOT available")

# other relevant statistics to filter datas depending on the address field

from collections import Counter
from collections import defaultdict

# function for extracting the address
def extract_city(hotel_info):
    address = hotel_info.get("Address", "")
    if "<span property=\"v:locality\">" in address:
        try:
            city = address.split("<span property=\"v:locality\">")[1].split("</span>")[0].strip()
            return city
        except IndexError:
            return "Unknown"
    return "Unknown"

# compute hotels' reviews
def count_reviews(hotel_data):
    reviews = hotel_data.get("Reviews", [])
    return len(reviews)

# dictionary to count the reviews' number per city
city_hotel_counter = defaultdict(int)
city_reviews_counter = defaultdict(int)


for file in os.listdir(path_to_json):
    if file.endswith('.json'):
        with open(os.path.join(path_to_json, file), 'r') as f:
            data = json.load(f)

            # get city from HotelInfo
            hotel_info = data.get("HotelInfo", {})
            city = extract_city(hotel_info)

            # count number of reviews
            num_reviews = count_reviews(data)

            # add it
            city_hotel_counter[city] += 1
            city_reviews_counter[city] += num_reviews

# print
print("city statistics:")
for city in city_hotel_counter:
    num_hotels = city_hotel_counter[city]
    total_reviews = city_reviews_counter[city]
    print(f"{city}: {num_hotels} Hotels, {total_reviews} total reviews")

"""Dataset Creation"""

# I chose to take into account only reviews of California cities. They should be enough to train a model for sentiment analysis

import os
import json
import pandas as pd

# division in 'positive' and 'negative' depending on the overall rating
def classify_sentiment(overall_rating):
    try:
        overall_rating = float(overall_rating)
        if overall_rating >= 4:
            return "Positive"
        elif overall_rating <= 2:
            return "Negative"
        else:
            return None  # leave out ratings of 3
    except ValueError:
        return None

# extract reviews and labels
def create_sentiment_dataset(path_to_json, target_cities):
    reviews_data = []

    for file in os.listdir(path_to_json):
        if file.endswith('.json'):
            with open(os.path.join(path_to_json, file), 'r') as f:
                data = json.load(f)


                hotel_info = data.get("HotelInfo", {})
                address = hotel_info.get("Address", "")
                if any(city in address for city in target_cities):
                    # iterate on reviews
                    for review in data.get("Reviews", []):
                        overall_rating = review.get("Ratings", {}).get("Overall")
                        sentiment = classify_sentiment(overall_rating)

                        if sentiment:  # add the instance in the dataset only if it is positive or negative (leave out 3 ratings)
                            reviews_data.append({
                                "Review": review.get("Content", ""),
                                "Sentiment": sentiment
                            })

    # dataframe creation
    df = pd.DataFrame(reviews_data)
    return df

# file path
path_to_json = "/content/drive/MyDrive/Business_Intelligence/BusinessIntel/DataErnesto/json"
target_cities = ["San Diego", "Los Angeles", "San Francisco", "Napa", "St. Helena", "Calistoga"] # are all in California

# dataset creation
dataset = create_sentiment_dataset(path_to_json, target_cities)

# save the dataset
dataset.to_csv("california_reviews.csv", index=False)

# visualize some lines
print(dataset.head())

# load CSV
dataset = pd.read_csv("california_reviews.csv")


positive_reviews = dataset[dataset["Sentiment"] == "Positive"].shape[0]
negative_reviews = dataset[dataset["Sentiment"] == "Negative"].shape[0]

# print statistics
print(f"positive reviews number: {positive_reviews}")
print(f"negative reviews number: {negative_reviews}")

# Downsampling to have a balanced dataset to work with

from sklearn.utils import resample

# Split the dataset
positive_reviews = dataset[dataset["Sentiment"] == "Positive"]
negative_reviews = dataset[dataset["Sentiment"] == "Negative"]

# positive class downsampling
positive_downsampled = resample(
    positive_reviews,
    replace=False,
    n_samples=len(negative_reviews),  # same number as negative
    random_state=42
)


balanced_dataset = pd.concat([positive_downsampled, negative_reviews])

# shuffle
balanced_dataset = balanced_dataset.sample(frac=1, random_state=42).reset_index(drop=True)

# verify
print("class distribution:")
print(balanced_dataset["Sentiment"].value_counts())

# create CSV to save it
balanced_dataset.to_csv("balanced_california_reviews.csv", index=False)

# print some lines
print(balanced_dataset.head())

"""# STEP 2: Sentiment Classification

1. Preprocess the text if needed (e.g., remove stop words, tokenize).
2. Create binary sentiment labels based on the rating:
o Positive if the rating is ≥ 4.
o Negative if the rating is ≤ 2.
3. Experiment with different machine learning models to classify the sentiment.
4. Select the best classifier based on appropriate evaluation metrics.
"""

import sklearn
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import make_pipeline
import random
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

import nltk

nltk.download('stopwords')

from nltk.corpus import stopwords

from sklearn.model_selection import train_test_split
import pandas as pd

# load CSV
balanced_dataset = pd.read_csv("balanced_california_reviews.csv")

# train set and test set division
train_set, test_set = train_test_split(
    balanced_dataset,
    test_size=0.10,  # 10% for test set
    random_state=42,
    stratify=balanced_dataset["Sentiment"]  # to mantain proportions
)

# verify
print(f"Train set size: {len(train_set)}")
print(f"Test set size: {len(test_set)}")


train_set.to_csv("train_california_reviews.csv", index=False)
test_set.to_csv("test_california_reviews.csv", index=False)

# visualize
print(train_set.head())

# create a TF-IDF Vectorizer
stop_words = set(stopwords.words('english'))
vectorizer = TfidfVectorizer(stop_words=list(stop_words), lowercase=False)

"""Random Forest on Token TF-IDF Vectorizer"""

# train a Random Forest classifier
classifier = RandomForestClassifier(random_state=42)
pipeline = make_pipeline(vectorizer, classifier)
pipeline.fit(train_set.Review, train_set.Sentiment)

# evaluate the model on the test data
test_predictions = pipeline.predict(test_set.Review)
accuracy = accuracy_score(test_set.Sentiment, test_predictions)
print("Accuracy:", accuracy)
print(
    "\nClassification Report:\n",
    classification_report(test_set.Sentiment,
                          test_predictions))

# confusion Matrix
conf_matrix = confusion_matrix(test_set["Sentiment"], test_predictions)
plt.figure(figsize=(10, 8))
sns.heatmap(conf_matrix,
            annot=True,
            fmt="d",
            cmap="Blues",
            xticklabels=["Negative", "Positive"],
            yticklabels=["Negative", "Positive"])
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title('Confusion Matrix')
plt.show()

"""**try also with other classifiers**

SVM
"""

from sklearn.svm import SVC

classifier = SVC(kernel="linear", C=1,probability=True, random_state=42)
# create pipeline with TF-IDF Vectorizer and SVM
svm_pipeline = make_pipeline(vectorizer, classifier)

# train the model
print("Training SVM...")
svm_pipeline.fit(train_set["Review"], train_set["Sentiment"])

# do predictions on test set
test_predictions = svm_pipeline.predict(test_set["Review"])

# evaluate the model
accuracy = accuracy_score(test_set["Sentiment"], test_predictions)
print(f"Accuracy: {accuracy:.4f}")

# classification report
print("\nClassification Report:")
print(classification_report(test_set["Sentiment"], test_predictions))

# confusion matrix
conf_matrix = confusion_matrix(test_set["Sentiment"], test_predictions)
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap="Blues", xticklabels=["Negative", "Positive"], yticklabels=["Negative", "Positive"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

"""Feed Forward Neural Network"""

from torch.utils.data import DataLoader, TensorDataset
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt

# convert sentiment labels to numeric values
label_encoder = LabelEncoder()
train_labels = label_encoder.fit_transform(train_set.Sentiment)
test_labels = label_encoder.transform(test_set.Sentiment)

# transform the text data into TF-IDF vectors
X_train = vectorizer.fit_transform(train_set.Review).toarray()
X_test = vectorizer.transform(test_set.Review).toarray() # using the fitting done for the training set

# convert data to PyTorch tensors
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)
test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)

# PyTorch dataset creation
train_dataset = TensorDataset(X_train_tensor, train_labels_tensor)
test_dataset = TensorDataset(X_test_tensor, test_labels_tensor)

# dataLoader creation
batch_size = 32
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# define the neural network class
class SentimentClassifier(nn.Module):
    def __init__(self, input_dim):
        super(SentimentClassifier, self).__init__()
        self.fc1 = nn.Linear(input_dim, 128)


        self.output = nn.Linear(128, 2)
        #self.dropout = nn.Dropout(0.5)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        #x = self.dropout(x)


        x = self.output(x)
        return x

# initialize the model, loss function, and optimizer
input_dim = X_train.shape[1]
model = SentimentClassifier(input_dim)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.0001)

print(input_dim) # neurons in the input layer

# training phase
num_epochs = 13
training_losses = []
validation_losses = []

for epoch in range(num_epochs):
    model.train()
    epoch_training_loss = 0.0

    # iter on train_loader batches
    for batch_X, batch_y in train_loader:
        optimizer.zero_grad()
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()
        epoch_training_loss += loss.item()

    # compute avg loss for this epoch
    epoch_training_loss /= len(train_loader)
    training_losses.append(epoch_training_loss)

    # validation loss computation
    model.eval()
    epoch_validation_loss = 0.0
    with torch.no_grad():
        for batch_X, batch_y in test_loader:
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            epoch_validation_loss += loss.item()

    epoch_validation_loss /= len(test_loader)
    validation_losses.append(epoch_validation_loss)

    print(f"Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_training_loss:.4f}, Validation Loss: {epoch_validation_loss:.4f}")

# evaluating the model
model.eval()
test_predictions = []
with torch.no_grad():
    for batch_X, _ in test_loader:
        outputs = model(batch_X)
        predictions = torch.argmax(outputs, axis=1)
        test_predictions.extend(predictions.numpy())

# accuracy metrics
accuracy = accuracy_score(test_labels, test_predictions)
print("Accuracy:", accuracy)
print("\nClassification Report:\n", classification_report(test_labels, test_predictions))

# losses polots (to check overfitting)
plt.figure(figsize=(10, 6))
plt.plot(range(1, num_epochs + 1), training_losses, label='Training Loss', marker='o')
plt.plot(range(1, num_epochs + 1), validation_losses, label='Validation Loss', marker='o')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.grid(True)
plt.show()

"""**BERT**"""

!pip install transformers datasets scikit-learn plotly InstructorEmbedding chromadb openai langchain replicate tiktoken sentence-transformers==2.2.2 huggingface_hub

# import necessary libraries
import torch
from transformers import DistilBertTokenizer, DistilBertModel, DistilBertForSequenceClassification, DistilBertForTokenClassification, pipeline
import chromadb
from chromadb.utils import embedding_functions

# load DistilBERT model
model = DistilBertModel.from_pretrained('distilbert-base-uncased')
# load DistilBERT tokenizer
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

from sklearn.manifold import TSNE
import numpy as np
import plotly.express as px
import matplotlib.pyplot as plt
import pandas as pd

# load dataset from CSV file
df = pd.read_csv("balanced_california_reviews.csv")

# tokenize and extract embeddings for each sentence
embeddings = []
labels = []
for i, row in df.iterrows():
    text = row['Review']
    label = row['Sentiment']
    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=512) # <- I cut off Reviews that are too long (in any case 512 already allows a fairly long text, and if I want to make sentiments I can understand if the text speaks well or badly about something)
    with torch.no_grad():
        embedding = model(**inputs).last_hidden_state.mean(dim=1).squeeze().numpy()
    embeddings.append(embedding)
    labels.append(label)

embeddings = np.array(embeddings)

# check how many reviews needed to be cut (I hope only a few)

from transformers import AutoTokenizer
import pandas as pd

# load the trained tokenizer
tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')

# load the dataset
df = pd.read_csv("balanced_california_reviews.csv")

# counter
long_reviews_count = 0

# iter trough the dataset
for i, row in df.iterrows():
    text = row['Review']

    # tokenize review
    inputs = tokenizer(text, return_tensors='pt', truncation=False, padding=True)

    # register instances with more than 512
    if inputs['input_ids'].shape[1] > 512:
        long_reviews_count += 1

# print
print(f"reviews with more than 512 token: {long_reviews_count}")

import numpy as np

# save embeddings and labels
np.save("embeddings.npy", embeddings)
np.save("labels.npy", labels)

import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score

# load embeddings and labels
embeddings = np.load("embeddings.npy")
labels = np.load("labels.npy")

# train and test set division (with embeddings and labels)
X_train, X_test, y_train, y_test = train_test_split(embeddings, labels, test_size=0.1, random_state=42)

"""Use BERT to train the neural network"""

from torch.utils.data import DataLoader, TensorDataset
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt

# convert sentiment labels to numeric values
label_encoder = LabelEncoder()
train_labels = label_encoder.fit_transform(y_train)
test_labels = label_encoder.transform(y_test)



# convert data to PyTorch tensors
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
train_labels_tensor = torch.tensor(train_labels, dtype=torch.long)
test_labels_tensor = torch.tensor(test_labels, dtype=torch.long)

# creation of PyTorch dataset
train_dataset = TensorDataset(X_train_tensor, train_labels_tensor)
test_dataset = TensorDataset(X_test_tensor, test_labels_tensor)

# dataloader creation
batch_size = 32
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# define the neural network
class SentimentClassifier(nn.Module):
    def __init__(self, input_dim):
        super(SentimentClassifier, self).__init__()
        self.fc1 = nn.Linear(input_dim, 128)


        self.output = nn.Linear(128, 2)
        #self.dropout = nn.Dropout(0.5)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        #x = self.dropout(x)


        x = self.output(x)
        return x

# initialize the model, loss function, and optimizer
input_dim = X_train.shape[1]
model = SentimentClassifier(input_dim)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.0001)

print(X_train.shape[1]) # very important to notice that now is decreased!!!!

# training phase
num_epochs = 13
training_losses = []
validation_losses = []

for epoch in range(num_epochs):
    model.train()
    epoch_training_loss = 0.0

    # iter on batches
    for batch_X, batch_y in train_loader:
        optimizer.zero_grad()
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()
        epoch_training_loss += loss.item()

    # compute avg loss
    epoch_training_loss /= len(train_loader)
    training_losses.append(epoch_training_loss)

    # compute validation loss
    model.eval()
    epoch_validation_loss = 0.0
    with torch.no_grad():
        for batch_X, batch_y in test_loader:
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            epoch_validation_loss += loss.item()

    epoch_validation_loss /= len(test_loader)
    validation_losses.append(epoch_validation_loss)

    print(f"Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_training_loss:.4f}, Validation Loss: {epoch_validation_loss:.4f}")

# evaluation
model.eval()
test_predictions = []
with torch.no_grad():
    for batch_X, _ in test_loader:
        outputs = model(batch_X)
        predictions = torch.argmax(outputs, axis=1)
        test_predictions.extend(predictions.numpy())

# accuracy
accuracy = accuracy_score(test_labels, test_predictions)
print("Accuracy:", accuracy)
print("\nClassification Report:\n", classification_report(test_labels, test_predictions))

# loss plots
plt.figure(figsize=(10, 6))
plt.plot(range(1, num_epochs + 1), training_losses, label='Training Loss', marker='o')
plt.plot(range(1, num_epochs + 1), validation_losses, label='Validation Loss', marker='o')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend()
plt.grid(True)
plt.show()

"""Accuracy is still very high but the training is faster because there are less weights to be trained. This make FFNN with BERT the best model, but let's see what happen on the explainability (XAI)

**Step 3: Explainable AI (XAI)**
1. Use XAI tools, as explained in class, to explain the predictions of the best classifier:
- Identify the features that most influence the sentiment classification.
2. Generate visualizations to make explanations understandable (e.g., feature
importance charts, heatmaps, or graphs).
"""

!pip install lime
import lime
from lime.lime_text import LimeTextExplainer

""" XAI for Random Forest"""

# RUN IT AFTER THE RANDOM FOREST TRAINING, WHITHOUT ANY INTERMEDIATE STEPS!!!

idx = 43
print(test_set.iloc[idx]["Review"])
print(test_set.iloc[idx]["Sentiment"])


# prediction for 43th instance
instance_43_text = test_set.iloc[idx]['Review']
predicted_class = pipeline.predict([instance_43_text])[0]
predicted_proba = pipeline.predict_proba([instance_43_text])[0]
print(f"Confidence Scores: {predicted_proba}")

# Create a LIME Text Explainer
explainer = LimeTextExplainer(class_names=["Negative", "Positive"],
                              random_state=42)

# Explain the prediction
exp = explainer.explain_instance(
    instance_43_text, pipeline.predict_proba, num_features=15)
exp.show_in_notebook(text=True)

"""XAI for SVM"""

# RUN IT AFTER SVM TRAINING, WITH NO INTERMEDIATE STEPS
idx = 43
print(test_set.iloc[idx]["Review"])
print(test_set.iloc[idx]["Sentiment"])


# prediction for 43th instance
instance_43_text = test_set.iloc[idx]['Review']
predicted_class = svm_pipeline.predict([instance_43_text])[0]
predicted_proba = svm_pipeline.predict_proba([instance_43_text])[0]
print(f"Confidence Scores: {predicted_proba}")

# create a LimeTextExplainer
explainer = LimeTextExplainer(class_names=["Negative", "Positive"],
                              random_state=42)

# select the review to be explained
instance_43_text = test_set.iloc[idx]['Review']

# function used by LIME to get probabilities
predict_fn = lambda x: svm_pipeline.predict_proba(x)

# explainer
exp = explainer.explain_instance(instance_43_text, predict_fn, num_features=10)

# visualize explanation
print(exp.as_list())
exp.show_in_notebook(text=instance_43_text)

"""XAI on FFNN and TF-IDF tokenizer"""

# RUN THE MODEL, THEN START
import torch.nn.functional as F

# 43th instance
idx = 43
instance_43_text = test_set.iloc[idx]["Review"]
true_sentiment = test_set.iloc[idx]["Sentiment"]

# transform into a TF-IDF vector
instance_43_vector = vectorizer.transform([instance_43_text]).toarray()
instance_43_tensor = torch.tensor(instance_43_vector, dtype=torch.float32)

# pass the vector to the model
model.eval()  # evaluation mode
with torch.no_grad():
    logits = model(instance_43_tensor)  # get logits
    probabilities = F.softmax(logits, dim=1)  # apply softmax to get probabilities
    predicted_class = torch.argmax(probabilities, dim=1).item()  # predicted class

# print results
print(f"Review: {instance_43_text}")
print(f"True Sentiment: {true_sentiment}")
print(f"Predicted Sentiment: {label_encoder.inverse_transform([predicted_class])[0]}")
print(f"Confidence Scores: {probabilities.numpy()}")

from lime.lime_text import LimeTextExplainer
import numpy as np
import torch.nn.functional as F

# model wrapper
class LimeWrapper:
    def __init__(self, model, vectorizer):
        self.model = model
        self.vectorizer = vectorizer

    def predict_proba(self, texts):
        # transform reviews in TF-IDF
        tfidf_vectors = self.vectorizer.transform(texts).toarray()
        tfidf_tensors = torch.tensor(tfidf_vectors, dtype=torch.float32)

        # pass data to the model
        self.model.eval()
        with torch.no_grad():
            logits = self.model(tfidf_tensors)
            probabilities = F.softmax(logits, dim=1)

        # get probabilities
        return probabilities.numpy()

# initialization of the wrapper
lime_model = LimeWrapper(model, vectorizer)

# create LimeTextExplainer
explainer = LimeTextExplainer(class_names=label_encoder.classes_)

# explain instance
explanation = explainer.explain_instance(
    instance_43_text,  # text to be explained
    lime_model.predict_proba,  # predicted probabilities function
    num_features=10,  # number of words to visualize
    labels=(0, 1)  # labels (negative and positive)
)

# visualize results
print("Explanation for class 0 (Negative):")
print(explanation.as_list(label=0))

print("\nExplanation for class 1 (Positive):")
print(explanation.as_list(label=1))


explanation.show_in_notebook(text=instance_43_text)

"""XAI for FFNN and BERT tokenizer"""

# RUN THE MODEL BEFORE
import torch.nn.functional as F

# prepare instance 43
idx = 43
instance_43_text = test_set.iloc[idx]["Review"]  # review text
true_sentiment = test_set.iloc[idx]["Sentiment"]  # true sentiment
# load DistilBERT model
modelBERT = DistilBertModel.from_pretrained('distilbert-base-uncased')
# load DistilBERT tokenizer
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

# get embeddings of 43th instance
inputs = tokenizer(instance_43_text, return_tensors="pt", truncation=True, max_length=512)
with torch.no_grad():
    embedding_43 = modelBERT(**inputs).last_hidden_state.mean(dim=1).squeeze()

# pass the embedding to the model
instance_43_tensor = embedding_43.unsqueeze(0)
model.eval()  # evaluation mode
with torch.no_grad():
    logits = model(instance_43_tensor)  # get the logit
    probabilities = F.softmax(logits, dim=1)  # apply softmax to get probabilities
    predicted_class = torch.argmax(probabilities, dim=1).item()  # predicted class

# print results
print(f"Review: {instance_43_text}")
print(f"True Sentiment: {true_sentiment}")
print(f"Predicted Sentiment: {label_encoder.inverse_transform([predicted_class])[0]}")
print(f"Confidence Scores: {probabilities.numpy()}")

from lime.lime_text import LimeTextExplainer
import numpy as np
import torch.nn.functional as F

# model wrapper
class LimeWrapperForEmbeddings:
    def __init__(self, model, tokenizer, bert_model, label_encoder):
        self.model = model
        self.tokenizer = tokenizer
        self.bert_model = bert_model
        self.label_encoder = label_encoder

    def predict_proba(self, texts):
        embeddings = []
        # compute the embeddings for each text (the review is still the same 43th, but step by step the review is perturbated leaving some words away, and the embedding is computed again)
        for text in texts:
            inputs = self.tokenizer(text, return_tensors='pt', truncation=True, max_length=512)
            with torch.no_grad():
                embedding = self.bert_model(**inputs).last_hidden_state.mean(dim=1).squeeze().numpy()
            embeddings.append(embedding)

        # convert in PyTorch tensors
        embeddings_tensor = torch.tensor(embeddings, dtype=torch.float32)

        # send embeddings to the model
        self.model.eval()
        with torch.no_grad():
            logits = self.model(embeddings_tensor)
            probabilities = F.softmax(logits, dim=1).numpy()

        return probabilities

# initialize the wrapper
lime_model = LimeWrapperForEmbeddings(model, tokenizer, modelBERT, label_encoder)

# initialize LimeTextExplainer
explainer = LimeTextExplainer(class_names=label_encoder.classes_)

# explain 43th instance
explanation = explainer.explain_instance(
    instance_43_text,  # text to be explained
    lime_model.predict_proba,  # function to set probabilities
    num_features=10,  # number of words to visualize
    labels=(0, 1)  # labels (negative or positive)
)

# visualize explanations
print("Explanation for class 0 (Negative):")
print(explanation.as_list(label=0))

print("\nExplanation for class 1 (Positive):")
print(explanation.as_list(label=1))


explanation.show_in_notebook(text=instance_43_text)

"""FFNN with BERT is the more complex and accurate model, but in terms of explainability it suffers from the stopwords and is in general less interpretable, because words thar humans retains to be important for such a classifications, here have less impact.
\
One should be aware of finding a good trade off between accuracy and explainability, depending on the various applications.
"""